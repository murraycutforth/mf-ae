{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "# In this notebook, we run inference and evaluation for each of the models on each of the other validation data sets, to assess the generalisation ability of each interface representation",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.interface_representation.interface_types import InterfaceType\n"
   ],
   "id": "cdb16f9425c9b393",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# First, assemble the 4 datasets which we want to use for evaluation\n",
    "# We want the paths to the dataset, exluding the final dir which specifies the interface type\n",
    "\n",
    "mu_sphere_1_base = Path('/Users/murray/Projects/multphase_flow_encoder/multiphase_flow_encoder/src/preprocessing/data/mu_spheres/spheres_mu_1.00')\n",
    "\n",
    "mu_sphere_2_base = Path('/Users/murray/Projects/multphase_flow_encoder/multiphase_flow_encoder/src/preprocessing/data/mu_spheres/spheres_mu_2.00')\n",
    "\n",
    "mu_sphere_25_base = Path('/Users/murray/Projects/multphase_flow_encoder/multiphase_flow_encoder/src/preprocessing/data/mu_spheres/spheres_mu_2.50')\n",
    "\n",
    "hit_base = Path('/Users/murray/Projects/multphase_flow_encoder/multiphase_flow_encoder/src/preprocessing/data/patched_hit_experiment')\n",
    "\n",
    "dataset_name_to_base = {\n",
    "    'spheres_mu_100': mu_sphere_1_base,\n",
    "    'spheres_mu_200': mu_sphere_2_base,\n",
    "    'spheres_mu_250': mu_sphere_25_base,\n",
    "    'hit': hit_base\n",
    "}\n",
    "\n",
    "dataset_interface_types = [\n",
    "    'TANH_EPSILON0.0078125',\n",
    "    'TANH_EPSILON0.015625',\n",
    "    'TANH_EPSILON0.03125',\n",
    "    'TANH_EPSILON0.0625',\n",
    "    'TANH_EPSILON0.125',\n",
    "    'TANH_EPSILON0.25',\n",
    "    'HEAVISIDE',\n",
    "    'SIGNED_DISTANCE_EXACT',\n",
    "]\n",
    "\n",
    "dataset_interface_types_to_interface_type = {\n",
    "    'TANH_EPSILON0.0078125': InterfaceType.TANH_EPSILON,\n",
    "    'TANH_EPSILON0.015625': InterfaceType.TANH_EPSILON,\n",
    "    'TANH_EPSILON0.03125': InterfaceType.TANH_EPSILON,\n",
    "    'TANH_EPSILON0.0625': InterfaceType.TANH_EPSILON,\n",
    "    'TANH_EPSILON0.125': InterfaceType.TANH_EPSILON,\n",
    "    'TANH_EPSILON0.25': InterfaceType.TANH_EPSILON,\n",
    "    'HEAVISIDE': InterfaceType.HEAVISIDE,\n",
    "    'SIGNED_DISTANCE_EXACT': InterfaceType.SIGNED_DISTANCE_EXACT,\n",
    "}\n",
    "\n",
    "dataset_interface_types_to_epsilon = {\n",
    "    'TANH_EPSILON0.0078125': 0.0078125,\n",
    "    'TANH_EPSILON0.015625': 0.015625,\n",
    "    'TANH_EPSILON0.03125': 0.03125,\n",
    "    'TANH_EPSILON0.0625': 0.0625,\n",
    "    'TANH_EPSILON0.125': 0.125,\n",
    "    'TANH_EPSILON0.25': 0.25,\n",
    "    'HEAVISIDE': None,\n",
    "    'SIGNED_DISTANCE_EXACT': None,\n",
    "}\n",
    "\n",
    "# Check all datasets are present, make sure directory exists\n",
    "\n",
    "datasets = []\n",
    "\n",
    "@dataclass\n",
    "class Dataset():\n",
    "    name: str\n",
    "    interface_type: InterfaceType\n",
    "    epsilon: float\n",
    "    path: Path\n",
    "\n",
    "for dataset_name, base in dataset_name_to_base.items():\n",
    "    for interface_type in dataset_interface_types:\n",
    "        path = base / interface_type\n",
    "        assert path.exists(), f'{path} does not exist'\n",
    "        \n",
    "        it = dataset_interface_types_to_interface_type[interface_type]\n",
    "        epsilon = dataset_interface_types_to_epsilon[interface_type]\n",
    "        \n",
    "        datasets.append(Dataset(dataset_name, it, epsilon, path))\n",
    "        \n",
    "# Given interface type and epsilon, map to a dict of dataset name to dataset path\n",
    "\n",
    "\n",
    "def get_datasets_given_interface(interface_type, epsilon, datasets):\n",
    "    matching_datasets = [d for d in datasets if d.interface_type == interface_type and d.epsilon == epsilon]\n",
    "    return matching_datasets\n",
    "\n"
   ],
   "id": "d263867633ea4f70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Next, assemble the paths to the models which we want to evaluate (these are v24 and v25)\n",
    "\n",
    "base_output_dir = Path('./../../output/lassen')\n",
    "files_v25 = list(base_output_dir.glob('interfacial_ae_v25_*'))\n",
    "files_v24 = list(base_output_dir.glob('interfacial_ae_v24_*'))\n",
    "\n",
    "print(len(files_v25))\n",
    "print(len(files_v24))\n",
    "\n",
    "# For each one, we want to associate with:\n",
    "# - The interface type\n",
    "# - The dataset name\n",
    "\n",
    "@dataclass\n",
    "class TrainedModel():\n",
    "    interface_type: InterfaceType\n",
    "    epsilon: float\n",
    "    dataset_name: str\n",
    "    model_path: Path\n",
    "    \n",
    "trained_models = []\n",
    "\n",
    "def extract_interface_type_v25(outdir):\n",
    "\n",
    "    final_part = '_'.join(outdir.stem.split('_')[8:-1])\n",
    "\n",
    "    final_part = final_part.replace('datadir', '')\n",
    "\n",
    "    str_to_type = {\n",
    "        'TANH_EPSILON00078125': InterfaceType.TANH_EPSILON,\n",
    "        'TANH_EPSILON0015625': InterfaceType.TANH_EPSILON,\n",
    "        'TANH_EPSILON003125': InterfaceType.TANH_EPSILON,\n",
    "        'TANH_EPSILON00625': InterfaceType.TANH_EPSILON,\n",
    "        'TANH_EPSILON0125': InterfaceType.TANH_EPSILON,\n",
    "        'TANH_EPSILON025': InterfaceType.TANH_EPSILON,\n",
    "        'HEAVISIDE': InterfaceType.HEAVISIDE,\n",
    "        'SIGNED_DISTANCE_EXACT': InterfaceType.SIGNED_DISTANCE_EXACT,\n",
    "    }\n",
    "\n",
    "    return str_to_type[final_part]\n",
    "\n",
    "def extract_epsilon_v25(outdir):\n",
    "    final_part = '_'.join(outdir.stem.split('_')[8:-1])\n",
    "    final_part = final_part.replace('datadir', '')\n",
    "    \n",
    "    str_to_epsilon = {\n",
    "        'TANH_EPSILON00078125': 0.0078125,\n",
    "        'TANH_EPSILON0015625': 0.015625,\n",
    "        'TANH_EPSILON003125': 0.03125,\n",
    "        'TANH_EPSILON00625': 0.0625,\n",
    "        'TANH_EPSILON0125': 0.125,\n",
    "        'TANH_EPSILON025': 0.25,\n",
    "        'HEAVISIDE': None,\n",
    "        'SIGNED_DISTANCE_EXACT': None,\n",
    "    }\n",
    "    \n",
    "    return str_to_epsilon[final_part]\n",
    "\n",
    "def extract_dataset_name_v25(outdir):\n",
    "    part = '_'.join(outdir.stem.split('_')[5:8])\n",
    "    part = part.replace('datadir', '')\n",
    "    return part\n",
    "\n",
    "def extract_model_path_v25(outdir):\n",
    "    model_path = outdir / 'model-100.pt'\n",
    "    assert model_path.exists()\n",
    "    return model_path\n",
    "\n",
    "def extract_interface_type_v24(outdir):\n",
    "\n",
    "    final_part = '_'.join(outdir.stem.split('_')[5:-1])\n",
    "    \n",
    "    final_part = final_part.replace('datadir', '')\n",
    "\n",
    "    str_to_type = {\n",
    "        'TANH_EPSILON00078125': InterfaceType.TANH_EPSILON,\n",
    "        'TANH_EPSILON0015625': InterfaceType.TANH_EPSILON,\n",
    "        'TANH_EPSILON003125': InterfaceType.TANH_EPSILON,\n",
    "        'TANH_EPSILON00625': InterfaceType.TANH_EPSILON,\n",
    "        'TANH_EPSILON0125': InterfaceType.TANH_EPSILON,\n",
    "        'TANH_EPSILON025': InterfaceType.TANH_EPSILON,\n",
    "        'HEAVISIDE': InterfaceType.HEAVISIDE,\n",
    "        'SIGNED_DISTANCE_EXACT': InterfaceType.SIGNED_DISTANCE_EXACT,\n",
    "    }\n",
    "\n",
    "    return str_to_type[final_part]\n",
    "\n",
    "def extract_epsilon_v24(outdir):\n",
    "    final_part = '_'.join(outdir.stem.split('_')[5:-1])\n",
    "    final_part = final_part.replace('datadir', '')\n",
    "    \n",
    "    str_to_epsilon = {\n",
    "        'TANH_EPSILON00078125': 0.0078125,\n",
    "        'TANH_EPSILON0015625': 0.015625,\n",
    "        'TANH_EPSILON003125': 0.03125,\n",
    "        'TANH_EPSILON00625': 0.0625,\n",
    "        'TANH_EPSILON0125': 0.125,\n",
    "        'TANH_EPSILON025': 0.25,\n",
    "        'HEAVISIDE': None,\n",
    "        'SIGNED_DISTANCE_EXACT': None,\n",
    "    }\n",
    "    \n",
    "    return str_to_epsilon[final_part]\n",
    "\n",
    "def extract_dataset_name_v24(outdir):\n",
    "    return 'hit'\n",
    "\n",
    "def extract_model_path_v24(outdir):\n",
    "    model_path = outdir / 'model-15.pt'\n",
    "    assert model_path.exists()\n",
    "    return model_path\n",
    "\n",
    "\n",
    "for f in files_v24:\n",
    "    interface_type = extract_interface_type_v24(f)\n",
    "    dataset_name = extract_dataset_name_v24(f)\n",
    "    model_path = extract_model_path_v24(f)\n",
    "    epsilon = extract_epsilon_v24(f)\n",
    "    \n",
    "    trained_models.append(TrainedModel(interface_type, epsilon, dataset_name, model_path))\n",
    "    \n",
    "for f in files_v25:\n",
    "    interface_type = extract_interface_type_v25(f)\n",
    "    dataset_name = extract_dataset_name_v25(f)\n",
    "    model_path = extract_model_path_v25(f)\n",
    "    epsilon = extract_epsilon_v25(f)\n",
    "    \n",
    "    trained_models.append(TrainedModel(interface_type, epsilon, dataset_name, model_path))\n",
    "    \n",
    "print(len(trained_models))\n"
   ],
   "id": "765d92f0ba58d166",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Iterate over every model / dataset combination\n",
    "# For each combination, run inference on the dataset, recording predictions and gt in dedicated results dir\n",
    "\n",
    "from eval_utils import load_dataset, load_model, run_inference\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "for tm in tqdm(trained_models, desc='Evaluating all trained models'):\n",
    "    \n",
    "    matching_datasets = get_datasets_given_interface(tm.interface_type, tm.epsilon, datasets)\n",
    "    \n",
    "    assert len(matching_datasets) == 4\n",
    "    \n",
    "    for dataset_info in matching_datasets:\n",
    "        print(f'Interface: {tm.interface_type}, {tm.epsilon}. Model trained on {tm.dataset_name}, evaluating on {dataset_info.name}')\n",
    "        \n",
    "        model = load_model(tm.model_path)\n",
    "        \n",
    "        if tm.dataset_name == 'hit':\n",
    "            dataset = load_dataset(dataset_info.path, max_num_samples=25_000)\n",
    "        else:\n",
    "            dataset = load_dataset(dataset_info.path)\n",
    "            \n",
    "        gts, preds = run_inference(dataset, model, N=50)\n",
    "        \n",
    "        outname = f'{tm.interface_type}_{tm.epsilon}_{tm.dataset_name}_{dataset_info.name}.npz'\n",
    "        outdir = Path('ood_results')\n",
    "        np.savez_compressed(outdir / outname, gts=gts, preds=preds, epsilon=tm.epsilon, interface_type=tm.interface_type, train_dataset_name=tm.dataset_name, eval_dataset_name=dataset_info.name)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ],
   "id": "66408878ca69c7d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ff3737177532474b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Now, let's compute dice scores for all these results\n",
    "# Construct a large pandas DataFrame where we store: interface label (type + epsilon), train dataset, eval dataset, mean dice score\n",
    "\n",
    "from eval_utils import get_phi_sharp, dice_coefficient\n",
    "from fractions import Fraction\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "result_rows = []\n",
    "\n",
    "\n",
    "def float_to_fraction(float_num):\n",
    "    fraction = Fraction(float(float_num)).limit_denominator()\n",
    "    return f\"{fraction.numerator}/{fraction.denominator}\"\n",
    "\n",
    "\n",
    "def get_interface_label(interface_type, epsilon):\n",
    "    if interface_type == InterfaceType.TANH_EPSILON:\n",
    "        return f'Tanh {float_to_fraction(epsilon)}'\n",
    "    elif interface_type == InterfaceType.HEAVISIDE:\n",
    "        return 'Sharp'\n",
    "    elif interface_type == InterfaceType.SIGNED_DISTANCE_EXACT:\n",
    "        return 'SDF'\n",
    "    elif interface_type == InterfaceType.SIGNED_DISTANCE_APPROXIMATE:\n",
    "        return 'SDF approx'\n",
    "    else:\n",
    "        raise ValueError('Unknown interface type')\n",
    "    \n",
    "    \n",
    "def adjust_dataset_labels(dataset_name):\n",
    "    if dataset_name == 'spheres_mu_100':\n",
    "        return 'Spheres ($\\\\mu=1.0$)'\n",
    "    elif dataset_name == 'spheres_mu_200':\n",
    "        return 'Spheres ($\\\\mu=2.0$)'\n",
    "    elif dataset_name == 'spheres_mu_250':\n",
    "        return 'Spheres ($\\\\mu=2.5$)'\n",
    "    elif dataset_name == 'hit':\n",
    "        return 'Droplet in HIT'\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset name')\n",
    "    \n",
    "\n",
    "for inference_result in Path('ood_results').glob('*.npz'):\n",
    "    data = np.load(inference_result, allow_pickle=True)\n",
    "    gts = data['gts']\n",
    "    preds = data['preds']\n",
    "    interface_type = data['interface_type']\n",
    "    \n",
    "    dice_scores = []\n",
    "    \n",
    "    for gt, pred in zip(gts, preds):\n",
    "        gt_sharp = get_phi_sharp(gt, interface_type)\n",
    "        pred_sharp = get_phi_sharp(pred, interface_type)\n",
    "        \n",
    "        dice = dice_coefficient(gt_sharp, pred_sharp)\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "    mean_dice = np.nanmean(dice_scores)\n",
    "    \n",
    "    result_rows.append({\n",
    "        'interface_label': get_interface_label(interface_type, data['epsilon']),\n",
    "        'train_dataset': adjust_dataset_labels(data['train_dataset_name']),\n",
    "        'eval_dataset': adjust_dataset_labels(data['eval_dataset_name']),\n",
    "        'mean_dice': mean_dice\n",
    "    })\n",
    "    \n",
    "df = pd.DataFrame(result_rows)\n",
    "\n",
    "# Now, for each interface_label, show a heatmap for train_dataset vs eval_dataset\n",
    "\n",
    "interface_labels = df['interface_label'].unique()\n",
    "\n",
    "# Sort interface labels\n",
    "def sort_key(interface_label):\n",
    "    if interface_label == 'Sharp':\n",
    "        return 0\n",
    "    elif interface_label == 'SDF':\n",
    "        return 100\n",
    "    else:\n",
    "        # Extract and sort by epsilon\n",
    "        return float(interface_label.split(' ')[1].split('/')[0]) / float(interface_label.split(' ')[1].split('/')[1])\n",
    "    \n",
    "interface_labels = sorted(interface_labels, key=sort_key)\n",
    "    \n",
    "print(interface_labels)\n",
    "\n",
    "subdfs = []\n",
    "\n",
    "for interface_label in interface_labels:\n",
    "    subdf = df[df['interface_label'] == interface_label]\n",
    "    subdf = subdf.drop(columns='interface_label')\n",
    "    subdf = subdf.pivot(index='train_dataset', columns='eval_dataset', values='mean_dice')\n",
    "    \n",
    "    subdfs.append((interface_label, subdf))\n",
    "    "
   ],
   "id": "e520a2cdcfc8259d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eaa2f7089dc9ff80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[df['interface_label'] == 'SDF']",
   "id": "ed73deec89f0f59a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d6cfadcf8fa8e69f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(8, 4), dpi=200, sharex=True, sharey=True)\n",
    "\n",
    "for i, (interface_label, subdf) in enumerate(subdfs):\n",
    "    ax = axs[i // 4, i % 4]\n",
    "    ax.set_title(interface_label)\n",
    "    im = ax.imshow(subdf, cmap='viridis', vmin=0.5, vmax=1)\n",
    "    ax.set_xticks(range(len(subdf.columns)))\n",
    "    ax.set_yticks(range(len(subdf.index)))\n",
    "    ax.set_xticklabels(subdf.columns, rotation=90)\n",
    "    ax.set_yticklabels(subdf.index)\n",
    "    \n",
    "axs[0, 0].set_ylabel('Train')\n",
    "axs[1, 0].set_ylabel('Train')\n",
    "axs[1, 0].set_xlabel('Test')\n",
    "axs[1, 1].set_xlabel('Test')\n",
    "axs[1, 2].set_xlabel('Test')\n",
    "axs[1, 3].set_xlabel('Test')\n",
    "    \n",
    "# Create colorbar on RHS\n",
    "cbar_ax = fig.add_axes([1, 0.15, 0.02, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax, label='Mean Dice Coefficient')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "fb3c28c315dd534c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
